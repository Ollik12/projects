{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e14883",
   "metadata": {},
   "source": [
    "# Exercise 4 -Computer Vision\n",
    "\n",
    "\n",
    "### 4.1 - Face Detection and Tracking\n",
    "In this task you will implement face detection and tracking using OpenCV. Specifically we are utilizing Cascade classifiers which implements Viola-Jones detection algorithm.\n",
    "\n",
    "**Reference**\n",
    "- [OpenCV documentation on cascade classifier](https://docs.opencv.org/master/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "### 4.1.1\n",
    "Execute the code below to initiate the cascadee classifier and the utility libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed45085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "import imutils\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb82336",
   "metadata": {},
   "source": [
    "### 4.1.2\n",
    "\n",
    "Similar to Task 3, the first step is to obtain a frame from video file and pre-processing it. \n",
    "\n",
    "**Your task**\n",
    "\n",
    "Complete prep() function below which performs following using opencv and imutils libraries. The steps already implemented are marked with a tick \"âœ“\"\n",
    "\n",
    "- [x] Takes a frame from video feed as the input\n",
    "- [ ] Resize the frame while protecting the aspect ratio (width = 600) \n",
    "- [ ] Flip the image\n",
    "- [ ] Convert the frame to grayscale image\n",
    "- [x] Return grayscale image and resized image \n",
    "\n",
    "**References**\n",
    "\n",
    "- [imutils documentation](https://github.com/PyImageSearch/imutils#imutils)\n",
    "- [Fip an array with OpenCV](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441)\n",
    "- [color conversion with OpenCV](https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce9f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(img):\n",
    "    ## ToDo 4.1.2\n",
    "    #  1. resize using  imutils.resize()\n",
    "    img = imutils.resize(img, width=600)\n",
    "    #  2. flip image vertically using cv2.flip() for this using 0 to flip vertically\n",
    "    # comment out flipping for better recognition\n",
    "    \n",
    "    # img = cv2.flip(img, 0)\n",
    "    \n",
    "    #  3. convert to gray color using cv2.cvtColor()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return gray, img    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96848eab",
   "metadata": {},
   "source": [
    "### 4.1.3\n",
    "\n",
    "In 4.1.1 we initialized an instance of cascade classifier. Tracking a face can be broken down into 3 steps as below\n",
    "\n",
    "1. Detect Faces and ROIs\n",
    "\n",
    "   The cascade classifier has a member function which can detect faces of multiple scales in a given image. The area where a face is detected becomes a region of interest (ROI) for extracting meaningful information. \n",
    "\n",
    "    **References** : \n",
    "    [Multiscale face detection member function of cascade classifier](https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html#a90fe1b7778bed4a27aa8482e1eecc116)\n",
    "\n",
    "\n",
    "2. Extract trackable features \n",
    "\n",
    "    Shi-Tomasi Corner Detector is an implementation in openCV which extracts information from the ROI input. The extracted information are points on the face which are are trackable across a sequence of moving frames (a video).\n",
    "\n",
    "    **References** : \n",
    "    [OpenCV Trackable feature extraction function(Shi-Tomasi Corner Detector)](https://docs.opencv.org/4.5.2/d4/d8c/tutorial_py_shi_tomasi.html)\n",
    "\n",
    "\n",
    "3. Calculate the optical flow\n",
    "\n",
    "    These trackable points are used to calculate the optical flow of the faces with calcOpticalFlowPyrLK() function. The tracking is visualized via OpenCV drawing tools.\n",
    "\n",
    "    **References** : \n",
    "    - [Optical Flow calculation](https://docs.opencv.org/4.5.3/d4/dee/tutorial_optical_flow.html)\n",
    "    - [OpenCV drawing functions](https://docs.opencv.org/4.5.2/dc/da5/tutorial_py_drawing_functions.html)\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Complete the function which perfoms following\n",
    "\n",
    "- [x] Takes grayscale image and resized image as the input\n",
    "- [x] Detect faces in graycale image using cascade classifier. detectMultiscale() function returns detected faces as rectangles ( Top left x coordinate, Top left y coordinate, width, height)\n",
    "- [ ] Draw a rectangle around detected faces using OpenCV drawing functions\n",
    "- [ ] Slice a region of interest (ROI) from grayecale image corresponding to the detections\n",
    "- [x] Extract good features to track (p0), from OpenCV goodFeaturesToTrack() function.\n",
    "- [ ] Convert the array p0 from current format [[[x1,y1],[x2,y2],....]] to --> [[x1,y1],[x2,y2],....]. Tip : print p0 to observe current format\n",
    "- [ ] The points are located with respect to the ROI coordinates. Convert them to image coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d3e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trackable_points(gray,img):\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "    all_points = []\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Draw rectangle when x y are top left corner\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Slice ROI\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect good features\n",
    "        p0 = cv2.goodFeaturesToTrack(roi_gray, maxCorners=70, qualityLevel=0.001, minDistance=5)\n",
    "        if p0 is not None:\n",
    "            p0 = p0.reshape(-1, 2)\n",
    "            # lets put ROI coordinates into image coordinates by adding the \n",
    "            # starting points of image where the slice is taken\n",
    "            p0[:,0] += x\n",
    "            p0[:,1] += y\n",
    "            all_points.append(p0)\n",
    "\n",
    "    # Combine all points into one array\n",
    "    if all_points:\n",
    "        p0 = np.vstack(all_points)\n",
    "    else:\n",
    "        p0 = np.array([])\n",
    "\n",
    "    return p0, faces, img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6c1c4",
   "metadata": {},
   "source": [
    "**Your task**\n",
    "\n",
    "Complete the do_track_face() function which perfoms following\n",
    "\n",
    "- [x] Usecv2.calcOpticalFlowPyrLK()to calculate the optical flow for tracking face\n",
    "- [ ] Select the valid points from p1. Note that  isFound == 1 for valid points \n",
    "- [ ] Return the valid points as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6754539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_track_face(gray_prev, gray, p0):\n",
    "    p1, isFound, err = cv2.calcOpticalFlowPyrLK(gray_prev, gray, p0, \n",
    "                                                            None,\n",
    "                                                            winSize=(31,31),\n",
    "                                                            maxLevel=10,\n",
    "                                                            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03),\n",
    "                                                            flags=cv2.OPTFLOW_LK_GET_MIN_EIGENVALS,\n",
    "                                                            minEigThreshold=0.00025)\n",
    "    ## ToDo 4.1.3 - Select valid points from p1\n",
    "    if p1 is not None and isFound is not None:\n",
    "        # flattening the p1\n",
    "        p1 = p1.reshape(-1, 2) # to get (N,2) shape\n",
    "        isFound = isFound.flatten() # to get (N,) shape\n",
    "        # so taking valid points from p1 and the ones with isFound == 1 are same index as good ones\n",
    "        valid_points = p1[isFound == 1] # are the ones with is found == 1\n",
    "        valid_points = np.array(valid_points)\n",
    "        return valid_points\n",
    "    return np.array([])  # return empty array if nothing found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740edad5",
   "metadata": {},
   "source": [
    "### 4.1.4\n",
    "\n",
    "Run the program to view the final output of face tracking. Remember to enter the correct path to video file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844eda03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18177/940778654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mframe_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mret_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frame_rate = 30\n",
    "prev = 0\n",
    "gray_prev = None\n",
    "p0 = []\n",
    "# original\n",
    "cam = cv2.VideoCapture(\"Face.mp4\")\n",
    "\n",
    "#webcam:\n",
    "# cam = cv2.VideoCapture(0)\n",
    "\n",
    "# for improvement\n",
    "frames_passed = 0\n",
    "if not cam.isOpened():\n",
    "    raise Exception(\"Could not open camera/file\")\n",
    "    \n",
    "while True:\n",
    "    time_elapsed = time.time() - prev\n",
    "    \n",
    "    if time_elapsed > 1./frame_rate:\n",
    "        \n",
    "        ret_val,img = cam.read()\n",
    "        \n",
    "        if not ret_val:\n",
    "                cam.set(cv2.CAP_PROP_POS_FRAMES, 0)  # restart video\n",
    "                gray_prev = None  # previous frame\n",
    "                p0 = []  # previous points\n",
    "                continue\n",
    "        prev = time.time()\n",
    "        \n",
    "        frames_passed += 1\n",
    "        \n",
    "        gray, img = prep(img)\n",
    "\n",
    "        if len(p0) <= 10 or frames_passed % 100 == 0: # update every 100 frames\n",
    "            p0, faces, img = get_trackable_points(gray,img)\n",
    "            gray_prev = gray.copy()\n",
    "        \n",
    "        else:\n",
    "            for (x,y,w,h) in faces:\n",
    "                cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                p1 = do_track_face(gray_prev, gray, p0)\n",
    "            for i in p1:\n",
    "                x, y = int(i[0]), int(i[1])\n",
    "                cv2.drawMarker(img, (x, y), (255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "            p0 = p1\n",
    "                   \n",
    "        cv2.imshow('Video feed', img)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "              \n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903f67c-957e-415c-9b44-c929378ec2f2",
   "metadata": {},
   "source": [
    "### Task5 implementation with the above Code\n",
    "\n",
    "Note - for this i have flipped the image so this will work better so for the demo just \n",
    "flipping it back again.\n",
    "\n",
    "then implementing new face regognition for \n",
    "\n",
    "turns out the best change including the new face regognition - the best fix was just not to flip the image \"shocking :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687bb973-d8a7-4034-9ce7-bff90f24e6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROS Humble (venv)",
   "language": "python",
   "name": "ros-humble-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
